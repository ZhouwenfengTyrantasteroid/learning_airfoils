{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../python')\n",
    "import time\n",
    "from plot_info import *\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import keras\n",
    "keras.backend.set_floatx('float64')\n",
    "import scipy.stats\n",
    "import h5py\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from print_table import *\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "random_seed = 42\n",
    "\n",
    "import numpy.random\n",
    "import tensorflow\n",
    "\n",
    "def seed_random_number(seed):\n",
    "    numpy.random.seed(seed)\n",
    "    tensorflow.set_random_seed(seed)\n",
    "    \n",
    "seed_random_number(random_seed)\n",
    "\n",
    "qmc_points = np.loadtxt('../sobol_6_8000.txt')\n",
    "qmc_points = qmc_points[1:].reshape((8000,6))\n",
    "\n",
    "all_points = qmc_points.copy()\n",
    "forces = np.array(np.loadtxt('../force_6_params.dat'))\n",
    "\n",
    "plt.scatter(forces[:,1], forces[:,2])\n",
    "plt.xlabel(\"Lift\")\n",
    "plt.ylabel(\"Drag\")\n",
    "plt.show()\n",
    "\n",
    "N = min(qmc_points.shape[0], forces.shape[0])\n",
    "qmc_points = qmc_points[:N,:]\n",
    "forces  = forces[:N,:]\n",
    "\n",
    "permuted_indices = range(N)\n",
    "qmc_points=qmc_points[permuted_indices,:]\n",
    "forces = forces[permuted_indices,:]\n",
    "input_size=6\n",
    "force_component = 1\n",
    "train_size=128\n",
    "validation_size=200\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_network(parameters, data, *,train_size, validation_size, batch_size, title, optimizer,\n",
    "               speedup_table, comparison_table, \n",
    "               wasserstein_table_builder,\n",
    "               bilevel_speedup_table,\n",
    "               prediction_error_table):\n",
    "    \n",
    "    best_network = None\n",
    "    best_network_index = None\n",
    "    best_learning_rate = None\n",
    "    best_weights = None\n",
    "    \n",
    "    \n",
    "    tries = 5\n",
    "    start_total_learning = time.time()\n",
    "    for trylearn in range(tries):\n",
    "        model = Sequential([\n",
    "            Dense(10, input_shape=(input_size,)),\n",
    "            Activation('relu'),\n",
    "            Dense(12),\n",
    "            Activation('relu'),\n",
    "            Dense(10),\n",
    "            Activation('relu'),\n",
    "            Dense(12),\n",
    "            Activation('relu'),\n",
    "            Dense(10),\n",
    "            Activation('relu'),\n",
    "            Dense(10),\n",
    "            Activation('relu'),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        \n",
    "       \n",
    "        \n",
    "       \n",
    "            \n",
    "        \n",
    "        model.compile(optimizer=optimizer(lr=0.01),\n",
    "                      loss='mean_squared_error')\n",
    "        \n",
    "        weights = np.copy(model.get_weights())\n",
    "        x_train = parameters[:train_size,:]\n",
    "        y_train=data[:train_size]\n",
    "        \n",
    "        \n",
    "        x_val = parameters[train_size:validation_size+train_size,:]\n",
    "        y_val=data[train_size:train_size+validation_size]\n",
    "        epochs=500000\n",
    "        \n",
    "        training_start_time=time.time()\n",
    "        hist = model.fit(x_train, y_train,batch_size=batch_size, epochs=epochs,shuffle=True, \n",
    "                         validation_data=(x_val, y_val),verbose=0)\n",
    "        training_end_time=time.time()\n",
    "        print(\"Training took {} seconds\".format (training_end_time-training_start_time))\n",
    "        console_log(\"Training took {} seconds\".format (training_end_time-training_start_time))\n",
    "        train_error = np.sum(hist.history['loss'][-max(epochs//1000,1):])\n",
    "        if best_network is None or train_error < best_learning_rate:\n",
    "            best_network = model\n",
    "            best_network_index = trylearn\n",
    "            best_learning_rate = train_error\n",
    "            best_weights = weights\n",
    "        plt.loglog(hist.history['loss'], label='Training loss')\n",
    "        plt.loglog(hist.history['val_loss'],label='Validation loss')\n",
    "        plt.legend()\n",
    "        plt.title('Training loss and validation loss (%d samples, batch size: %d)\\n%s' %(train_size, batch_size, title))\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.show()\n",
    "            \n",
    "    \n",
    "            \n",
    "    end_total_learning = time.time()\n",
    "    \n",
    "    print(\"Best network index: %d\" % best_network_index)\n",
    "    console_log(\"Best network index: %d\" % best_network_index)\n",
    "    print(\"Total learning time took: %d s\" % (end_total_learning-start_total_learning))\n",
    "    console_log(\"Total learning time took: %d s\" % (end_total_learning-start_total_learning))\n",
    "    model = best_network\n",
    "    print_keras_model_as_table('airfoilnetwork', model)\n",
    "    weights = best_weights\n",
    "    \n",
    "    # save model to file, see https://machinelearningmastery.com/save-load-keras-deep-learning-models/\n",
    "    model_json = model.to_json()\n",
    "    with open(\"results/\" + showAndSave.prefix + \"model.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights(\"results/\" + showAndSave.prefix + \"model.h5\")\n",
    "    np.save(\"results/\" + showAndSave.prefix + \"intial.npy\", weights)\n",
    "    \n",
    "    epochs_r=range(1, epochs)\n",
    "    plt.loglog(hist.history['loss'])\n",
    "    plt.title('Training loss (%d samples, batch size: %d)\\n%s' %(train_size, batch_size, title))\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    showAndSave('training_loss')\n",
    "    \n",
    "    plt.loglog(hist.history['val_loss'])\n",
    "    plt.title('Validation loss (%d samples, batch size: %d)\\n%s' %(train_size, batch_size, title))\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    showAndSave('validation_loss')\n",
    "    \n",
    "    \n",
    "    \n",
    "    plt.loglog(hist.history['loss'], label='Training loss')\n",
    "    plt.loglog(hist.history['val_loss'],label='Validation loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training loss and validation loss (%d samples, batch size: %d)\\n%s' %(train_size, batch_size, title))\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    showAndSave('training_validation_loss')\n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    x_test =  parameters[validation_size+train_size:,:]\n",
    "    y_test = data[train_size+validation_size:]\n",
    "    y_predict = model.predict(x_test)\n",
    "    \n",
    "    plt.title('Scatter comparision (%d samples, batch size: %d)\\n%s' %(train_size, batch_size, title))\n",
    "    plt.scatter(y_test, y_predict)\n",
    "    plt.xlabel(\"Actual data\")\n",
    "    plt.ylabel(\"Predicted data\")\n",
    "    showAndSave(\"scatter_comparison\")\n",
    "    print(model.summary())\n",
    "    \n",
    "   \n",
    "    from sklearn import linear_model\n",
    "    reg = linear_model.LinearRegression()\n",
    "    coeffs = reg.fit(parameters[:train_size,:], y_train)\n",
    "    \n",
    "    evaluated_lsq = coeffs.predict(parameters)\n",
    "    plt.scatter(data, evaluated_lsq)\n",
    "    plt.title('Linear Least squares (%d samples)' % (train_size))\n",
    "    plt.xlabel(\"Actual data\")\n",
    "    plt.ylabel(\"Interpolated data\")\n",
    "    showAndSave(\"scatter_lsq_comparision\")\n",
    "    \n",
    "    \n",
    "    def myvar(x):\n",
    "        mean = np.sum(x)/x.shape[0]\n",
    "        var = np.sum((mean-x)**2)/x.shape[0]\n",
    "        return var\n",
    "        \n",
    "    def mymean (x): \n",
    "        return np.sum(x)/x.shape[0]\n",
    "    \n",
    "    variance_top = myvar(data)\n",
    "    print(\"variance single level = %f\" % variance_top)\n",
    "    predicted = model.predict(parameters)\n",
    "    predicted = predicted.reshape(parameters.shape[0])\n",
    "    variance_diff_ml = myvar(data - predicted)\n",
    "    \n",
    "    \n",
    "    print(\"variance diff ml = %f \" % variance_diff_ml)\n",
    "    print(\"speedup = %f\" % (variance_top/variance_diff_ml))\n",
    "    bilevel_speedup_table.set_header([\"Functional\", \"DLbQMC Speedup\"])\n",
    "    bilevel_speedup_table.add_row([title, variance_top/variance_diff_ml])\n",
    "    print((data - evaluated_lsq).shape)\n",
    "    variance_diff_interpolate =myvar(data - evaluated_lsq)\n",
    "    print(\"variance_diff_interpolate = %f\" % variance_diff_interpolate)\n",
    "    \n",
    "    \n",
    "    \n",
    "    mean_qmc = np.mean(data)\n",
    "    print(\"mean_qmc = %f\" % mean_qmc)\n",
    "    mean_ml = np.mean(model.predict(parameters))\n",
    "    print(\"mean_ml = %f\" % mean_ml)\n",
    "    mean_few_qmc = np.mean(parameters[:train_size,:])\n",
    "    \n",
    "    print(\"mean_few_qmc = %f\" % mean_few_qmc)\n",
    "    predicted_all = model.predict(all_points)\n",
    "    predicted_all = predicted_all.reshape(all_points.shape[0])\n",
    "    print(predicted_all.shape)\n",
    "    mean_mlmlmc = mymean(predicted[:train_size]-data[:train_size]) + mymean(predicted_all)\n",
    "    \n",
    "    print(\"mean_mlmlmc = %f\" % mean_mlmlmc)\n",
    "    var_qmc = np.var(data)\n",
    "    print(\"var_qmc = %f\" % var_qmc)\n",
    "    var_ml = np.var(model.predict(parameters))\n",
    "    print(\"var_ml = %f\" % var_ml)\n",
    "    var_few_qmc = np.var(parameters[:train_size,:])\n",
    "    \n",
    "    print(\"var_few_qmc = %f\" % var_few_qmc)\n",
    "    print(parameters.shape)\n",
    "    \n",
    "    \n",
    "   \n",
    "    plt.hist(data,bins=40,density=True,label='QMC %d samples' % parameters.shape[0],alpha=0.5)\n",
    "    plt.title(\"Comparison QMC and DLQMC\\n%s\\nepochs=%d,batch_size=%d\"% (title, epochs,batch_size))\n",
    "    plt.hist(model.predict(parameters),bins=40,density=True,label='DLQMC (%d samples)' % train_size,alpha=0.5)\n",
    "    plt.legend()\n",
    "    showAndSave('hist_qmc_ml_%s' % title)\n",
    "    \n",
    "    \n",
    "    plt.title(\"Comparison QMC with %d and QMC with %d samples\\n%s\" %(parameters.shape[0], train_size, title))\n",
    "    plt.hist(data,bins=40,density=True,label='QMC %d samples' % parameters.shape[0],alpha=0.5)\n",
    "    plt.hist(data[:train_size],bins=40,density=True, alpha=0.5,label='QMC %d samples' % train_size)\n",
    "    plt.legend()\n",
    "    showAndSave('hist_qmc_qmc_%s' % title)\n",
    "    \n",
    "    plt.title(\"Comparison QMC with least squares\\n%s\" % title)\n",
    "    plt.hist(data,bins=40,density=True,label='QMC %d samples' % train_size,alpha=0.5)\n",
    "    plt.hist(evaluated_lsq,bins=40,density=True,alpha=0.5, label='Least squares (%d points)' % train_size)\n",
    "    plt.legend()\n",
    "    showAndSave('hist_qmc_lsq_%s' % title)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    plt.hist(data,bins=20,density=True,label='QMC %d samples' % parameters.shape[0],alpha=0.5)\n",
    "    plt.title(\"(coarse hist) Comparison QMC and DLQMC\\n%s\\nepochs=%d,batch_size=%d\"% (title, epochs,batch_size))\n",
    "    plt.hist(model.predict(parameters),bins=20,density=True,label='DLQMC(%d samples)' % train_size,alpha=0.5)\n",
    "    plt.legend()\n",
    "    showAndSave('hist_qmc_ml_coarse_%s' % title)\n",
    "    \n",
    "    \n",
    "    plt.title(\"(coarse hist) Comparison QMC with %d and QMC with %d samples\\n%s\" %(parameters.shape[0], train_size, title))\n",
    "    plt.hist(data,bins=20,density=True,label='QMC %d samples' % parameters.shape[0],alpha=0.5)\n",
    "    plt.hist(data[:train_size],bins=20,density=True, alpha=0.5,label='QMC %d samples' % train_size)\n",
    "    plt.legend()\n",
    "    showAndSave('hist_qmc_qmc_coarse_%s' % title)\n",
    "    \n",
    "    plt.title(\"(coarse hist) Comparison QMC with least squares\\n%s\" % title)\n",
    "    plt.hist(data,bins=20,density=True,label='QMC %d samples' % train_size,alpha=0.5)\n",
    "    plt.hist(evaluated_lsq,bins=20,density=True,alpha=0.5, label='Least squares (%d points)' % train_size)\n",
    "    plt.legend()\n",
    "    showAndSave('hist_qmc_lsq_coarse_%s' % title)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    prediction_error = np.sum(keras.backend.eval(keras.losses.mean_squared_error(data, model.predict(parameters))))/data.shape[0]\n",
    "    prediction_error_lsq = np.sum(keras.backend.eval(keras.losses.mean_squared_error(data, evaluated_lsq)))/data.shape[0]\n",
    "    \n",
    "    print(prediction_error)\n",
    "    \n",
    "    \n",
    "    prediction_error_table.set_header([\"Functional\", \"Deep learning\", \"Least squares\"])\n",
    "    prediction_error_table.add_row([title, prediction_error, prediction_error_lsq])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    samples = range(0,data.shape[0])\n",
    "    stats = {}\n",
    "    for stat in ['mean', 'var']:\n",
    "        stats[stat]={}\n",
    "        stats[stat]['sources']={}\n",
    "        if stat == 'mean':\n",
    "            stats[stat]['compute']=lambda x: sum(x)/x.shape[0]\n",
    "        else:\n",
    "            stats[stat]['compute']=lambda x: sum(x**2)/x.shape[0]-(sum(x)/x.shape[0])**2\n",
    "    \n",
    "     \n",
    "        stats[stat]['sources']['QMC']={}\n",
    "        stats[stat]['sources']['DLQMC'] = {}\n",
    "        stats[stat]['sources']['Least squares'] = {}\n",
    "        stats[stat]['sources']['DLbQMC'] = {}\n",
    "    \n",
    "        stats[stat]['sources']['QMC']['data']=array([stats[stat]['compute'](data[:k]) for k in samples])\n",
    "        stats[stat]['sources']['DLQMC']['data'] = array([stats[stat]['compute'](array(model.predict(parameters[:k,:]))) for k in samples])\n",
    "        stats[stat]['sources']['Least squares']['data'] = array([stats[stat]['compute'](evaluated_lsq[:k]) for k in samples])\n",
    "        \n",
    "        stats[stat]['sources']['DLbQMC']['data'] = [0]\n",
    "        \n",
    "        for k in samples[1:]:\n",
    "            if stat == 'mean':\n",
    "                mean = sum(model.predict(parameters[:train_size,:])-data[:train_size])/train_size +\\\n",
    "                sum(model.predict(parameters[:k,:]))/k\n",
    "                \n",
    "\n",
    "                stats[stat]['sources']['DLbQMC']['data'].append(mean)\n",
    "            elif stat=='var':\n",
    "                mean = sum(model.predict(parameters[:train_size,:])-data[:train_size])/train_size +\\\n",
    "                sum(model.predict(parameters[:k,:]))/k\n",
    "                \n",
    "                m2 = sum((data[:train_size])**2-(model.predict(parameters[:train_size,:]))**2)/train_size +\\\n",
    "                sum(model.predict(parameters[:k,:])**2)/k\n",
    "                \n",
    "\n",
    "                stats[stat]['sources']['DLbQMC']['data'].append(m2-mean**2)\n",
    "                \n",
    "        stats[stat]['sources']['DLbQMC']['data']=array(stats[stat]['sources']['DLbQMC']['data'])\n",
    "        \n",
    "        sources = stats[stat]['sources'].keys()\n",
    "        for source in sources:\n",
    "            \n",
    "            stats[stat]['sources'][source]['representative'] = stats[stat]['sources'][source]['data'][-1]\n",
    "            \n",
    "        \n",
    "       \n",
    "    \n",
    "        for source in stats[stat]['sources'].keys():\n",
    "            if 'DLbQMC' not in source:\n",
    "                plt.plot(samples, stats[stat]['sources'][source]['data'], label=source)\n",
    "        plt.xlabel('Number of samples ($J_L$)')\n",
    "        plt.ylabel('%s' % stat)\n",
    "        plt.title('%s as a function of number of samples used for evaluation\\n%s' % (stat, title))\n",
    "        plt.legend()\n",
    "        showAndSave('function_of_samples_airfoil_%s_%s'  % (stat, title))\n",
    "        stats[stat]['sources']['QMC %d' % train_size] = {}\n",
    "        stats[stat]['sources']['QMC %d' % train_size]['representative'] = stats[stat]['sources']['QMC']['data'][train_size]\n",
    "    sources = [source for source in stats['mean']['sources'].keys()]\n",
    "    datatable = [[],[],[]]\n",
    "    for source in sources:\n",
    "        if source != sources[-1]:\n",
    "            sys.stdout.write('\\\\multicolumn{2}{c|}{\\\\textbf{%s}} &&' % source)\n",
    "        else:\n",
    "            sys.stdout.write('\\\\multicolumn{2}{c|}{\\\\textbf{%s}} \\\\\\\\ \\n' % source)\n",
    "        \n",
    "        datatable[0].append(source)\n",
    "    comparison_table.set_upper_header(datatable[0])\n",
    "    sys.stdout.write(\"&\")\n",
    "    for source in sources:\n",
    "        for stat in ['mean', 'var']:\n",
    "            if not (stat == 'var' and source ==  sources[-1]):\n",
    "                sys.stdout.write(\"%s &\" % stat)\n",
    "            else:\n",
    "                sys.stdout.write(\"%s \\\\\\\\ \" % stat)\n",
    "                \n",
    "            if stat == 'var' and source != sources[-1]:\n",
    "                sys.stdout.write(\"& \")\n",
    "            else:\n",
    "                sys.stdout.write(\" \")\n",
    "            datatable[1].append(stat)\n",
    "    comparison_table.set_lower_header(datatable[1])\n",
    "    sys.stdout.write(\"%s & \" % title)\n",
    "    for source in sources:\n",
    "        for stat in ['mean', 'var']:\n",
    "            if not (stat == 'var' and source ==  sources[-1]):\n",
    "                sys.stdout.write(\"%.5f &\" % stats[stat]['sources'][source]['representative'])\n",
    "            else:\n",
    "                sys.stdout.write(\"%.5f \\\\\\\\ \\n\" % stats[stat]['sources'][source]['representative'])\n",
    "            \n",
    "            if stat == 'var' and source != sources[-1]:\n",
    "                sys.stdout.write(\"& \")\n",
    "            else:\n",
    "                sys.stdout.write(\" \")\n",
    "            datatable[2].append(stats[stat]['sources'][source]['representative'])\n",
    "            \n",
    "            \n",
    "    comparison_table.add_row([title]+datatable[2])\n",
    "    print_comparison_table('comparison_stats', datatable, multicolumn=True)\n",
    "    \n",
    "    \n",
    "    #### Speedup\n",
    "    speeduptable = [[],[],[]]\n",
    "    statstouse = ['mean', 'var']\n",
    "    baseline='QMC'\n",
    "    small_baseline = 'QMC %d' % train_size\n",
    "    competitors = ['QMC %d' % train_size, 'DLQMC', 'DLbQMC', 'Least squares']\n",
    "    sys.stdout.write(\"&\")\n",
    "    for competitor in competitors:\n",
    "        speeduptable[0].append(competitor)\n",
    "        if competitor != competitors[-1]:\n",
    "            sys.stdout.write('\\\\multicolumn{2}{c|}{\\\\textbf{%s}} &&' % competitor)\n",
    "        else:\n",
    "            sys.stdout.write('\\\\multicolumn{2}{c|}{\\\\textbf{%s}} \\\\\\\\ \\n' % competitor)\n",
    "    speedup_table.set_upper_header(speeduptable[0])\n",
    "    sys.stdout.write(\"&\")\n",
    "    for source in competitors:\n",
    "        for stat in ['mean', 'var']:\n",
    "            speeduptable[1].append(stat)\n",
    "            if not (stat == 'var' and competitor ==  competitors[-1]):\n",
    "                sys.stdout.write(\"%s &\" % stat)\n",
    "            else:\n",
    "                sys.stdout.write(\"%s \\\\\\\\ \" % stat)\n",
    "                \n",
    "            if stat == 'var' and competitor != competitors[-1]:\n",
    "                sys.stdout.write(\"& \")\n",
    "            else:\n",
    "                sys.stdout.write(\" \")\n",
    "    speedup_table.set_lower_header(speeduptable[1])\n",
    "    sys.stdout.write(\"%s & \" % title)\n",
    "    \n",
    "    for competitor in competitors:\n",
    "        for stat in ['mean', 'var']:\n",
    "            \n",
    "            console_log(competitor)\n",
    "            console_log(stat)\n",
    "            console_log(stats[stat])\n",
    "            console_log(stats[stat]['sources'])\n",
    "            console_log(stats[stat]['sources'][competitor])\n",
    "            console_log(stats[stat]['sources'][baseline])\n",
    "            console_log(stats[stat]['sources'][competitor]['representative'])\n",
    "            \n",
    "            \n",
    "            error = abs(stats[stat]['sources'][competitor]['representative']-\\\n",
    "                        stats[stat]['sources'][baseline]['representative'])\n",
    "            \n",
    "            error_base = abs(stats[stat]['sources'][small_baseline]['representative']-\\\n",
    "                             stats[stat]['sources'][baseline]['representative'])\n",
    "            \n",
    "            speedup = error_base/error\n",
    "            if not (stat == 'var' and competitor ==  competitors[-1]):\n",
    "                sys.stdout.write(\"%.5f &\" % speedup)\n",
    "            else:\n",
    "                sys.stdout.write(\"%.5f \\\\\\\\ \\n\" % speedup)\n",
    "            \n",
    "            if stat == 'var' and source != sources[-1]:\n",
    "                sys.stdout.write(\"& \")\n",
    "            else:\n",
    "                sys.stdout.write(\" \")\n",
    "            speeduptable[2].append(speedup)\n",
    "    speedup_table.add_row([title] + speeduptable[2])\n",
    "    print_comparison_table('comparison_speedup', speeduptable, multicolumn=True)\n",
    "    for stat in ['mean', 'var']:\n",
    "        errors_qmc = []\n",
    "            \n",
    "        for k in samples[1:-2]:\n",
    "            errors_qmc.append(abs(stats[stat]['sources']['QMC']['representative']-\\\n",
    "                                      stats[stat]['sources']['QMC']['data'][k]))\n",
    "        plt.loglog(samples[1:-2], errors_qmc, label='QMC error')\n",
    "        plt.axvline(x=train_size, linestyle='--', color='grey')\n",
    "        \n",
    "        for competitor in ['DLbQMC', 'DLQMC', 'Least squares']:\n",
    "            error = abs(stats[stat]['sources'][competitor]['representative']-stats[stat]['sources'][baseline]['representative'])\n",
    "            \n",
    "                \n",
    "            \n",
    "            plt.loglog(samples[1:-2], error*ones_like(samples[1:-2]), '--', label='%s error' % competitor)\n",
    "        \n",
    "        plt.xlabel('Number of samples for QMC')\n",
    "        plt.ylabel('Error')\n",
    "        plt.title('Error for %s compared to QMC\\n%s' % (stat, title))\n",
    "        plt.legend()\n",
    "        showAndSave(\"error_evolution_%s\" % stat)\n",
    "                      \n",
    "                \n",
    "                \n",
    "    ###### Speedup Wasserstein\n",
    "    #first we fill the last 23 values with the last value:\n",
    "    data_modified = []\n",
    "    for k in data:\n",
    "        data_modified.append(k)\n",
    "    for k in range(2*2**(int(log2(data.shape[0])))-data.shape[0]):\n",
    "        data_modified.append(data[-1])\n",
    "    data_modified = np.array(data_modified)\n",
    "    N_wasser = 2**(int(log2(data_modified.shape[0])))\n",
    "    data_wasser = data_modified[:N_wasser]\n",
    "    qmc_upscaled = repeat(data[:train_size], N_wasser/train_size)\n",
    "    \n",
    "    wasser_qmc_qmc = scipy.stats.wasserstein_distance(data_wasser, qmc_upscaled)\n",
    "    wasser_qmc_ml = scipy.stats.wasserstein_distance(data_wasser, reshape(model.predict(parameters), data.shape))\n",
    "    wasser_qmc_lsq = scipy.stats.wasserstein_distance(data_wasser, evaluated_lsq)\n",
    "    \n",
    "    speedup_qmc = wasser_qmc_qmc / wasser_qmc_qmc\n",
    "    speedup_ml =  wasser_qmc_qmc / wasser_qmc_ml \n",
    "    speedup_lsq = wasser_qmc_qmc / wasser_qmc_lsq\n",
    "    \n",
    "    wasserstein_table=[[\"DLQMC\", \"Least squares\", \"QMC 128\"],[speedup_ml, speedup_lsq, speedup_qmc]]\n",
    "    print_comparison_table(\"wasserstein\", wasserstein_table)\n",
    "    \n",
    "    wasserstein_table_builder.set_header(wasserstein_table[0])\n",
    "    wasserstein_table_builder.add_row([title]+wasserstein_table[1])\n",
    "    \n",
    "    print(\"\\\\toprule\")\n",
    "    print(\"&DLQMC & Least squares & QMC 128\\\\\\\\\")\n",
    "    print(\"\\\\midrule\")\n",
    "    print(\"{} & {} & {} & {}\".format(title, speedup_ml, speedup_lsq, speedup_qmc))\n",
    "    \n",
    "    \n",
    "    errors_qmc = []\n",
    "            \n",
    "    for k in range(1, int(log2(data_modified.shape[0]))):\n",
    "        qmc_upscaled = repeat(data_modified[:int(2**k)], N_wasser//int(2**k))\n",
    "        errors_qmc.append(scipy.stats.wasserstein_distance(data_wasser, qmc_upscaled))\n",
    "        \n",
    "    samples_wasser = 2**array(range(1, int(log2(data_modified.shape[0]))))\n",
    "    plt.loglog(samples_wasser, errors_qmc, '-o', label='QMC error')\n",
    "    \n",
    "    plt.loglog(samples_wasser, wasser_qmc_ml*ones_like(samples_wasser), '--', label='DLMC error')\n",
    "    plt.loglog(samples_wasser, wasser_qmc_lsq*ones_like(samples_wasser), '--', label='LSQ error')\n",
    "    plt.axvline(x=train_size, linestyle='--', color='grey')\n",
    "    plt.xlabel('Number of samples for QMC')\n",
    "    plt.ylabel('Error (Wasserstein)')\n",
    "    plt.title('Error (Wasserstein) compared to QMC\\n%s' % title)\n",
    "    plt.legend()\n",
    "    showAndSave(\"error_evolution_wasserstein\")\n",
    "            \n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One network per variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "force_names=['Lift', 'Drag']\n",
    "batch_sizes=[train_size]\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    speedup_table = TableBuilder()\n",
    "    comparison_table = TableBuilder()\n",
    "    wasserstein_table_builder = TableBuilder()\n",
    "    bilevel_speedup_table= TableBuilder()\n",
    "    prediction_error_table = TableBuilder()\n",
    "    for (n, f) in enumerate(force_names):\n",
    "        showAndSave.prefix='airfoil_%s_ts_%d_bs_%d' %(f,batch_size, train_size)\n",
    "        network= get_network(qmc_points, forces[:,n+1], train_size=train_size, validation_size=validation_size,\n",
    "                        batch_size=batch_size, title=f, optimizer=keras.optimizers.SGD,\n",
    "                            speedup_table=speedup_table,\n",
    "                            comparison_table=comparison_table,\n",
    "                            wasserstein_table_builder=wasserstein_table_builder,\n",
    "                            bilevel_speedup_table=bilevel_speedup_table,\n",
    "                            prediction_error_table=prediction_error_table)\n",
    "    showAndSave.prefix='airfoil_all_ts_%d_bs_%d' %(batch_size, train_size)\n",
    "    speedup_table.print_table('speedup')\n",
    "    comparison_table.print_table('comparison')\n",
    "    wasserstein_table_builder.print_table(\"wasserstein\")\n",
    "    bilevel_speedup_table.print_table(\"bilevel_speedup\")\n",
    "    prediction_error_table.print_table(\"prediction_error_table\")\n",
    "display(HTML(\"<h1>Adams</h1>\"))\n",
    "seed_random_number(random_seed)\n",
    "for batch_size in batch_sizes:\n",
    "    speedup_table = TableBuilder()\n",
    "    comparison_table = TableBuilder()\n",
    "    wasserstein_table_builder = TableBuilder()\n",
    "    bilevel_speedup_table= TableBuilder()\n",
    "    prediction_error_table = TableBuilder()\n",
    "    for (n, f) in enumerate(force_names):\n",
    "        showAndSave.prefix='airfoil_adams_%s_ts_%d_bs_%d' %(f,batch_size, train_size)\n",
    "        network= get_network(qmc_points, forces[:,n+1], train_size=train_size, validation_size=validation_size,\n",
    "                        batch_size=batch_size, title=f, optimizer=keras.optimizers.Adam,\n",
    "                            speedup_table=speedup_table,\n",
    "                            comparison_table=comparison_table,\n",
    "                            wasserstein_table_builder=wasserstein_table_builder,\n",
    "                            bilevel_speedup_table=bilevel_speedup_table,\n",
    "                            prediction_error_table=prediction_error_table)\n",
    "        \n",
    "    showAndSave.prefix='airfoil_all_adams_ts_%d_bs_%d' %(batch_size, train_size)\n",
    "    speedup_table.print_table('speedup')\n",
    "    comparison_table.print_table('comparison')\n",
    "    wasserstein_table_builder.print_table(\"wasserstein\")\n",
    "    bilevel_speedup_table.print_table(\"bilevel_speedup\")\n",
    "    prediction_error_table.print_table(\"prediction_error_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
